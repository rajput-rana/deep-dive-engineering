# Amazon S3 Deep Dive for System Design Interviews

Walk into any system design interview involving file storage, and S3 will almost certainly come up.
The reason is simple: S3 solves the hard problems of storing unstructured data at scale, durability, availability, and cost optimization, so you can focus on building your application.
It looks deceptively simple: you PUT an object, you GET an object, and it “just scales.” That simplicity is exactly why S3 shows up everywhere—from storing profile photos and video segments to backups, data lakes, logs, and machine learning datasets. 
But the real value of S3 isn’t the API. It’s the set of guarantees and trade-offs behind it: durability through replication, availability through distributed control planes, performance shaped by key design and request patterns, and cost shaped by lifecycle policies and storage classes. 
If you only know “S3 stores files,” you’ll miss the design decisions that make systems reliable and efficient at scale.
This chapter will give you the depth to discuss S3 confidently in interviews.
# 1. When to Choose S3
Every storage decision in a system design interview needs justification. Interviewers want to see that you understand *why* a particular storage solution fits your requirements, not just that you know it exists.
S3 is object storage, which means it stores data as discrete objects (files) rather than as blocks (like EBS) or in a hierarchical file system (like EFS). This distinction shapes when S3 is the right choice.

### 1.1 S3 Shines When You Have

#### Unstructured data at scale
Images, videos, documents, logs, backups, anything that's essentially a file. S3 can store individual objects up to 5 TB, and there's no limit on the total number of objects in a bucket.

#### Write-once, read-many workloads
Think about how users interact with photos on Instagram or videos on YouTube. Once uploaded, the content doesn't change; it just gets read millions of times. S3 is optimized for exactly this pattern.

#### Need for durability without complexity
S3 automatically replicates your data across at least three availability zones within a region. This gives you 11 nines of durability (99.999999999%), meaning if you store 10 million objects, you'd statistically lose one every 10,000 years. You get this without configuring anything.

#### Variable or unpredictable scale
S3 currently stores over 350 trillion objects and handles 100+ million requests per second globally. More importantly, it scales automatically. You never provision capacity or worry about running out of space.

#### Cost optimization matters
With seven different storage classes, you can match storage costs to access patterns. Data accessed once a month costs 45% less than frequently accessed data. Data accessed once a year costs 95% less.
The diagram below captures the core decision process:

### 1.2 When S3 is the Wrong Choice
Understanding S3's limitations is just as important as knowing its strengths. Here's where S3 falls short:

#### You need low-latency lookups
S3 operations typically have latency in the tens to hundreds of milliseconds range. If you need single-digit millisecond reads (like session data or real-time counters), use DynamoDB or ElastiCache instead.

#### Your data changes frequently in small increments
S3 objects are immutable. To update a file, you must upload an entirely new version. If you're updating a user's score every second or appending to a log file continuously, a database or append-optimized storage is more appropriate.

#### Applications expect file system semantics
S3 doesn't support file locking, in-place modifications, or POSIX operations like `seek` and `append`. Legacy applications that mount storage as a file system won't work with S3 directly. Use EFS or FSx for these workloads.

#### You need transactions across multiple objects
S3 has no concept of transactions. You cannot atomically update three objects such that either all succeed or all fail. For transactional workloads, use a database with proper ACID guarantees.

### 1.3 S3 in Common Interview Scenarios
Understanding where S3 fits in real systems helps you make convincing arguments in interviews:
| System | How S3 Is Used | Key Features Leveraged |
| --- | --- | --- |
| Video Streaming (Netflix, YouTube) | Store original uploads and transcoded versions | Multipart upload, storage class tiering, CDN integration |
| Photo Sharing (Instagram) | Store images and thumbnails | Pre-signed URLs, lifecycle policies, cross-region replication |
| File Sync (Dropbox) | Backend storage for user files | Versioning, strong consistency, chunked uploads |
| Data Lake | Raw data for analytics pipelines | S3 Select, Athena integration, storage classes |
| Log Aggregation | Centralized storage from distributed systems | Lifecycle policies to archive/delete, partitioned prefixes |
| Backup/DR | Point-in-time backups, disaster recovery | Cross-region replication, Glacier for long-term retention |

When you propose S3 in an interview, don't just say "we'll use S3 for storage." Specify which features address your requirements. 
**For example:** "We'll use S3 with lifecycle policies to automatically move videos older than 30 days to Infrequent Access storage, reducing costs by 45%." This shows you understand both the technology and its practical application.
# 2. S3 Architecture Fundamentals
To use S3 effectively in system design, you need to understand how it organizes and stores data. This knowledge helps you design efficient key structures, avoid performance bottlenecks, and answer architecture questions confidently.

### 2.1 Core Concepts
S3 organizes data using three fundamental concepts:
**Buckets** are the top-level containers. Think of a bucket as a namespace for your objects. Bucket names must be globally unique across all AWS accounts (not just yours), which is why you often see names like `company-name-environment-purpose`. Each bucket exists in a specific AWS region, though the data can be replicated elsewhere.
**Objects** are the actual files you store. Each object consists of:
- **Key**: The unique identifier within the bucket (essentially the file path)
- **Value**: The data itself, up to 5 TB per object
- **Metadata**: Key-value pairs for system info (content-type, size) and custom data
- **Version ID**: If versioning is enabled, each version of an object gets a unique ID

**Keys** deserve special attention because they're often misunderstood. A key like `users/123/photos/vacation.jpg` looks like a folder path, but S3 has no concept of folders. The entire string is a flat key. The "/" characters are just part of the name, though the AWS console renders them as folders for convenience.
Here's how an S3 object is structured:

### 2.2 Internal Architecture
While AWS doesn't publish S3's exact implementation details, we know enough about its architecture to understand how it achieves its performance and durability guarantees.
When you make an S3 request, it flows through several layers:
**Request Router** receives incoming API requests and handles load balancing across S3's infrastructure.
**Authentication/Authorization** validates your credentials and checks policies (IAM, bucket policies, ACLs) before allowing the request to proceed.
**Index Service** maintains metadata about every object: where it's stored, its size, checksum, and version information. This is the brain of S3.
**Storage Nodes** hold the actual object data. Data is automatically replicated across at least three availability zones for durability.

### 2.3 Data Distribution and Partitioning
One of S3's most powerful features is automatic partitioning. Unlike DynamoDB where you need to carefully design partition keys to avoid hot spots, S3 handles this for you.
S3 partitions data based on key prefixes. Objects with similar prefixes tend to land on the same partition, which enables efficient listing operations.
When a partition becomes "hot" (receiving too many requests), S3 automatically splits it into smaller partitions. This happens transparently, you don't need to do anything.
Each prefix can sustain 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second. For most applications, this is more than enough. When you need higher throughput, you can distribute objects across multiple prefixes (more on this in the performance section).

### 2.4 Request Flow
Let's trace what happens when your application reads an object from S3:
1. **DNS Resolution**: Your request to `bucket-name.s3.region.amazonaws.com` resolves to an S3 endpoint
2. **Authentication**: S3 validates your credentials (signature, timestamps, etc.)
3. **Authorization**: Policies are evaluated (IAM policies, bucket policies, ACLs, Block Public Access)
4. **Routing**: The request is routed to the storage nodes holding your data
5. **Data Retrieval**: Object is fetched and returned

This entire flow is designed for 99.99% availability (Standard storage class) and 99.999999999% durability. The durability comes from automatic replication across availability zones, the availability comes from redundant infrastructure at every layer.
# 3. Storage Classes and Cost Optimization
Storage is often the largest infrastructure cost for data-heavy applications. A video platform might spend millions per month on S3. Understanding storage classes isn't just an interview topic, it's a skill that can save real money.
S3 offers seven storage classes, each optimized for different access patterns. The key insight is that storage costs and retrieval costs have an inverse relationship: cheaper storage means more expensive or slower retrieval.

### 3.1 Storage Class Overview
The storage classes fall into three categories based on how frequently you access the data:
Here's a detailed comparison:
| Storage Class | Best For | Retrieval Time | Min Storage Duration | Cost/GB/month |
| --- | --- | --- | --- | --- |
| S3 Standard | Active data accessed frequently | Instant | None | $0.023 |
| Intelligent-Tiering | Unknown or changing access patterns | Instant | None | $0.023 + monitoring fee |
| Standard-IA | Data accessed monthly, needs fast retrieval | Instant | 30 days | $0.0125 |
| One Zone-IA | Reproducible data, cost-sensitive | Instant | 30 days | $0.01 |
| Glacier Instant | Quarterly access, needs immediate retrieval | Milliseconds | 90 days | $0.004 |
| Glacier Flexible | Archive accessed 1-2 times per year | 1-12 hours | 90 days | $0.0036 |
| Glacier Deep Archive | Compliance, rarely accessed | 12-48 hours | 180 days | $0.00099 |

### 3.2 Choosing the Right Storage Class
The decision comes down to three questions:
**1. How often is the data accessed?**
Access frequency is the primary driver. If you're unsure, start with Standard and use S3 Storage Class Analysis (a built-in tool) to understand your actual access patterns.
**2. How quickly do you need retrieval?**
This is the trade-off. Glacier Deep Archive costs 95% less than Standard, but you might wait 48 hours to retrieve your data. For a backup you hope to never need, that's acceptable. For thumbnails on a photo sharing app, it's not.
**3. Can the data be recreated?**
One Zone-IA stores data in a single availability zone, which means a zone outage could cause data loss. This is fine for transcoded video outputs (you can re-transcode from the original) but not for user uploads (those are irreplaceable).

### 3.3 Lifecycle Policies
Manually moving objects between storage classes doesn't scale. If you have millions of objects, you need automation. That's what lifecycle policies provide.
A lifecycle policy is a set of rules that tell S3: "After X days, move objects matching this prefix to this storage class" or "After Y days, delete these objects."
Here's a practical example for application logs:
The savings compound as your data grows. At petabyte scale, this can mean millions of dollars saved annually.

### 3.4 S3 Intelligent-Tiering
What if you don't know your access patterns? This is common with user-generated content. Some photos get viewed constantly; others are uploaded and never accessed again.
Intelligent-Tiering solves this by automatically moving objects between tiers based on actual access patterns. You don't configure rules; S3 monitors access and optimizes automatically.
The trade-off is a small monitoring fee: $0.0025 per 1,000 objects per month. For a million objects, that's $2.50/month. In exchange, you get automatic optimization without managing lifecycle rules.
Intelligent-Tiering works best when you have large amounts of data with unpredictable access patterns and when the cost of incorrect manual tiering would exceed the monitoring fee.
# 4. Data Consistency Model
Consistency models determine what guarantees you have when reading data after writing it. This matters when multiple processes or services interact with the same S3 objects, a common scenario in distributed systems.

### 4.1 Strong Read-After-Write Consistency
Until December 2020, S3 had a reputation for eventual consistency on overwrites. If you wrote a new version of an object, a subsequent read might return the old version. This caused subtle bugs and forced developers to add workarounds.
That changed in December 2020. S3 now provides **strong read-after-write consistency for all operations**, at no additional cost.
This is a significant improvement. You no longer need to add delays, use external coordination, or design around eventual consistency when using S3.

### 4.2 How Strong Consistency Works
When you write an object to S3, the operation doesn't return success until the data is durably stored across multiple availability zones *and* the index is updated. Only then does the client receive a success response. This ensures any subsequent read will see the new data.
**What S3 guarantees:**
- A GET after a successful PUT always returns the new object
- A GET after a successful DELETE always returns 404
- LIST operations immediately reflect PUTs and DELETEs

**What S3 does NOT guarantee:**
- Ordering between concurrent writes (last writer wins, but "last" is determined by S3, not your timestamps)
- Atomic updates across multiple objects (no transactions)

### 4.3 Concurrent Write Handling
S3 does not provide locking or conditional writes (like DynamoDB's conditional expressions). When two clients write to the same key simultaneously, the last write wins, where "last" means whichever write completes final in S3's system.
This behavior isn't a bug; it's a design choice that enables S3's massive scale. If you need coordination between writers, you have a few options:
1. **Enable versioning**: Both versions are preserved; you can reconcile later
2. **Use external coordination**: DynamoDB or a lock service to serialize writes
3. **Design around it**: Use unique keys (e.g., include a timestamp or UUID) so writes never conflict

### 4.4 Versioning for Data Protection
Versioning is one of S3's most valuable features for data protection. When enabled, S3 keeps every version of every object. Overwrites create new versions; deletes add delete markers instead of removing data.
When you delete a versioned object, S3 adds a "delete marker" instead of actually deleting the data. A regular GET returns 404 (the delete marker wins), but the old versions remain accessible by version ID.
This protects against accidental deletions, application bugs, and even malicious actors (if combined with MFA Delete). For critical data, versioning should be your default.
# 5. Performance Optimization
When interviewers ask about S3 performance, they want to know you understand both S3's baseline capabilities and how to scale beyond them. Let's cover both.

### 5.1 Understanding S3's Performance Limits
S3's performance is measured per prefix, not per bucket. Each prefix can handle:
- **3,500 PUT/COPY/POST/DELETE** requests per second
- **5,500 GET/HEAD** requests per second

These limits are substantial. For context, 5,500 GET requests per second means you could serve 475 million reads per day from a single prefix. Most applications never hit these limits.
But when you do hit them, understanding how to scale is critical.

### 5.2 Scaling with Prefix Distribution
The key insight is that limits apply *per prefix*. If you distribute objects across multiple prefixes, you multiply your throughput proportionally.
Here's a practical approach: instead of storing all user avatars under `avatars/user123.jpg`, add a hash prefix:
Using a hex character as the first directory gives you 16 prefixes and 16x throughput. Use two hex characters for 256x. The trade-off is slightly more complex key generation logic.

### 5.3 Multipart Upload
Uploading a 5 GB video as a single request is fragile. If the connection drops at 4.9 GB, you start over. Multipart upload solves this by splitting large files into independently uploadable parts.
**How it works:**
1. **Initiate**: Call `CreateMultipartUpload`, receive an upload ID
2. **Upload parts**: Upload each part independently (in parallel if you want), receive an ETag for each
3. **Complete**: Call `CompleteMultipartUpload` with all ETags; S3 combines the parts

**Why it's better:**
- **Parallel uploads**: Upload 10 parts simultaneously for 10x throughput
- **Retry granularity**: If part 7 fails, retry only part 7, not the entire file
- **Resumability**: Stop mid-upload, resume later with the same upload ID
- **Required for large files**: Objects over 5 GB *must* use multipart upload

AWS recommends multipart upload for any object over 100 MB. The SDK handles this automatically if you use the high-level transfer utilities.

### 5.4 S3 Transfer Acceleration
When your users are uploading to an S3 bucket on the other side of the world, the public internet becomes the bottleneck. Transfer Acceleration addresses this by routing uploads through CloudFront's global edge network.
The improvement comes from two factors: the edge location is geographically close to the user (lower first-hop latency), and AWS's backbone network is faster and more reliable than the public internet.
Typical improvements range from 50-500% for cross-region transfers, with the biggest gains for intercontinental uploads. The cost is an additional $0.04-0.08 per GB transferred.

### 5.5 Byte-Range Fetches
You don't always need the entire object. S3 supports HTTP range requests, allowing you to fetch specific byte ranges.
**Use cases:**
- **Video previews**: Fetch only the first few megabytes to show a preview
- **Resume failed downloads**: Start from where you left off
- **Parallel downloads**: Split a large file across multiple threads

This technique can dramatically improve download speeds, especially for large files over high-latency connections.

### 5.6 Performance Summary
| Technique | When to Use | Impact |
| --- | --- | --- |
| Prefix distribution | Request rate exceeds 5,500/s per prefix | Multiply throughput by number of prefixes |
| Multipart upload | Objects > 100 MB | Parallel upload, retry individual parts |
| Transfer Acceleration | Cross-region or intercontinental transfers | 50-500% faster uploads |
| Byte-range fetches | Large files, partial reads, parallel downloads | Reduced latency, resumable downloads |
| CloudFront caching | Read-heavy, globally distributed users | Lower latency, reduced S3 request costs |

In interviews, the most important techniques to mention are prefix distribution (for high-throughput scenarios) and multipart upload (for large files). These come up in nearly every S3-related system design question.
# 6. Security and Access Control
S3 buckets have been the source of countless data breaches, often due to misconfiguration rather than vulnerabilities. Understanding S3's security model isn't optional; it's essential for any system design involving sensitive data.
S3 provides multiple layers of access control, and they work together. A request must pass through all applicable layers to succeed.

### 6.1 The Access Control Evaluation Flow
When a request arrives, S3 evaluates policies in a specific order. An explicit deny at any layer blocks the request.
The key principle: **explicit deny wins**. If any policy explicitly denies the request, it's blocked, regardless of what other policies allow.

### 6.2 IAM Policies
IAM policies attach to principals (users, groups, or roles) and define what AWS actions they can perform. These are *identity-based* policies; they answer "What can this identity do?"
Notice the two resource entries: `ListBucket` operates on the bucket itself, while `GetObject` operates on objects within the bucket. Missing either results in access denied for that operation.

### 6.3 Bucket Policies
Bucket policies attach to the bucket and define who can access it. These are *resource-based* policies; they answer "Who can access this resource?"
The key difference from IAM policies: bucket policies can grant access to principals outside your AWS account, including other AWS accounts or even the public internet.
For cross-account access to work, *both* accounts need to grant permission: the resource owner (via bucket policy) and the accessing account (via IAM policy on the role/user).

### 6.4 S3 Block Public Access
Block Public Access is a safety net that overrides other access controls. Even if a bucket policy or ACL grants public access, Block Public Access can prevent it.
This feature was introduced after numerous high-profile data breaches from misconfigured S3 buckets. It's now enabled by default for new buckets.
There are four settings, each blocking a different vector for public access:
| Setting | What It Blocks |
| --- | --- |
| BlockPublicAcls | Uploading objects with public ACLs |
| IgnorePublicAcls | Ignores existing public ACLs on the bucket |
| BlockPublicPolicy | Prevents bucket policies that grant public access |
| RestrictPublicBuckets | Limits access to AWS services even if policy allows public |

**Best practice**: Enable all four settings at the account level, then disable only on specific buckets that genuinely need public access (like static website hosting).

### 6.5 Encryption
Data in S3 should be encrypted at rest. The question isn't whether to encrypt, but how to manage the encryption keys.
| Type | Key Management | Trade-offs |
| --- | --- | --- |
| SSE-S3 | AWS manages keys completely | Simplest, no additional cost, no audit trail |
| SSE-KMS | Keys in AWS KMS, you control policies | Audit trail in CloudTrail, automatic rotation, additional cost |
| SSE-C | You provide key with each request | Full control, but you manage key distribution |
| Client-side | You encrypt before upload | AWS never sees plaintext, but more complex implementation |

For most applications, **SSE-S3** is sufficient. Use **SSE-KMS** when you need audit trails (CloudTrail logs every key usage) or fine-grained access control on the keys themselves. Client-side encryption is for high-security scenarios where you don't want AWS to ever see the plaintext data.

### 6.6 Pre-Signed URLs
Pre-signed URLs are one of S3's most useful features for application development. They allow you to grant temporary access to a private object without sharing AWS credentials or making the bucket public.
A pre-signed URL includes authentication information in the URL itself: a signature, the operation allowed (GET or PUT), and an expiration time.
**Common use cases:**
- **Direct uploads**: Client uploads large files directly to S3, bypassing your server. This saves bandwidth and reduces latency.
- **Temporary downloads**: Share a private file with someone for a limited time (e.g., "download link expires in 24 hours").
- **CDN origin authentication**: CloudFront uses pre-signed URLs to access private S3 content.

Pre-signed URLs are a powerful pattern for any application where users upload or download files. They appear frequently in system design interviews for file storage, media upload, and document sharing systems.
# 7. Replication and Durability
Durability and replication are different concepts, though they're often conflated. Durability is about not losing data. Replication is about having copies in multiple locations for availability, latency, or compliance reasons.
S3 provides exceptional durability by default, and optional replication features for cross-region scenarios.

### 7.1 Default Durability: 11 Nines
S3 Standard storage class provides 99.999999999% durability. This number is so high it's worth putting in perspective:
S3 achieves this through:
- **Multi-AZ replication**: Data is automatically stored across at least 3 availability zones
- **Continuous integrity checking**: S3 regularly verifies data integrity using checksums
- **Automatic repair**: Corrupted or lost data is automatically restored from replicas
- **No single point of failure**: Every component is redundant

### 7.2 Cross-Region Replication (CRR)
While S3's default durability protects against data loss within a region, Cross-Region Replication (CRR) copies data to a different AWS region entirely. This protects against region-wide disasters and enables several important use cases.
**Why use CRR:**
- **Disaster recovery**: If an entire AWS region becomes unavailable, your data exists elsewhere
- **Compliance**: Some regulations require data to be stored in specific geographic locations
- **Latency reduction**: Replicate data closer to users in other regions
- **Data aggregation**: Collect logs from multiple regions into a central bucket

**Requirements:**
- Both source and destination buckets must have versioning enabled
- An IAM role with permission to replicate objects
- Source and destination must be in different regions (that's the "cross-region" part)

You can configure CRR to replicate the entire bucket or only objects matching specific prefixes, to change the storage class during replication, and to optionally replicate delete markers.

### 7.3 Same-Region Replication (SRR)
Same-Region Replication copies objects to another bucket in the same region. Why would you want this if the data stays in the same region?
**Use cases:**
- **Log aggregation**: Collect logs from multiple application buckets into a central analytics bucket
- **Environment isolation**: Replicate production data to a test environment for debugging
- **Account separation**: Keep a copy of data in a different AWS account for governance
- **Backup with different permissions**: Replicate to a bucket with stricter deletion policies

### 7.4 Replication Time Control (RTC)
Standard replication is best-effort; most objects replicate within minutes, but there's no SLA. Replication Time Control adds predictability.
With RTC, S3 guarantees that 99.99% of objects will replicate within 15 minutes. You also get metrics to monitor replication lag, which is essential for compliance scenarios.
Use RTC when your disaster recovery SLA depends on data freshness, or when compliance requires you to prove replication timing.

### 7.5 S3 Object Lock
Object Lock prevents objects from being deleted or overwritten, even by the account owner. This is write-once-read-many (WORM) storage for S3.
**Governance mode** allows users with specific IAM permissions to override the lock. Use this for testing retention policies or cases where you need an escape hatch.
**Compliance mode** is truly immutable. Once set, no one can delete the object until the retention period expires. Not administrators, not the root user, not AWS support. This is for regulatory requirements like SEC 17a-4 (financial records) or HIPAA (healthcare records).
Object Lock also supports **legal holds**, which prevent deletion indefinitely until the hold is explicitly removed. This is used for litigation or investigation scenarios.
# 8. S3 vs Other Storage Solutions
Knowing when *not* to use S3 is as important as knowing when to use it. Interviewers often present scenarios where S3 seems like the obvious choice, then probe whether you understand its limitations.

### 8.1 S3 vs EBS (Elastic Block Store)
EBS is block storage that attaches to EC2 instances. Think of it as a virtual hard drive.
| Aspect | S3 | EBS |
| --- | --- | --- |
| Type | Object storage (files) | Block storage (disk) |
| Access | HTTP API from anywhere | Attached to single EC2 instance |
| Latency | 50-100ms typical | Sub-millisecond |
| Durability | 99.999999999% | 99.999% |
| Max size | 5 TB per object, unlimited total | 64 TB per volume |

**Choose S3** when: You need shared access, data is files/documents, or you're building for web-scale.
**Choose EBS** when: You need a database, boot volume, or any workload requiring low-latency disk I/O.

### 8.2 S3 vs EFS (Elastic File System)
EFS is a managed NFS file system that multiple EC2 instances can mount simultaneously.
| Aspect | S3 | EFS |
| --- | --- | --- |
| Type | Object storage | POSIX file system |
| Access | REST API | Mount as file system |
| Sharing | Via SDK/HTTP | Mounted by many instances |
| Latency | 50-100ms | 1-10ms |
| Operations | PUT/GET whole objects | Read, write, append, seek |

**Choose S3** when: You're storing static files, serving content via CDN, or building data lakes.
**Choose EFS** when: Applications expect a traditional file system, you need POSIX operations, or multiple servers need shared writable storage.

### 8.3 S3 Standard vs Glacier
Glacier is technically a set of S3 storage classes, but it's worth understanding the trade-off explicitly.
| Aspect | S3 Standard | S3 Glacier Deep Archive |
| --- | --- | --- |
| Access time | Immediate | 12-48 hours |
| Storage cost | $0.023/GB/month | $0.00099/GB/month |
| Retrieval cost | Free | $0.02/GB |
| Minimum duration | None | 180 days |

The math is simple: if you access data less than once a year and can wait 12+ hours for retrieval, Glacier Deep Archive costs 95% less. But if you need that data urgently, waiting 48 hours for retrieval can be unacceptable.

### 8.4 Cross-Cloud Comparison
All major clouds have object storage with similar capabilities:
| Feature | AWS S3 | GCP Cloud Storage | Azure Blob Storage |
| --- | --- | --- | --- |
| Max object size | 5 TB | 5 TB | 4.75 TB |
| Storage tiers | 7 | 4 | 4 |
| Consistency | Strong | Strong | Strong |
| Unique strength | Most integrations, most mature | BigQuery integration | Azure AD integration |

In practice, choose based on your cloud provider. If you're multi-cloud, consider an abstraction layer, but accept the trade-off of losing provider-specific features.
# 9. Applying S3 Knowledge in Interviews
Knowing S3's features is necessary but not sufficient. You need to apply this knowledge to design realistic systems. Here's how S3 fits into common interview scenarios.

### 9.1 File Storage System (Dropbox/Google Drive)
When designing a file sync service, S3 is the natural choice for the storage layer. Here's how to structure your answer:
**Key design decisions:**
- **Chunking**: Split files into chunks (4-8 MB each) for efficient sync. Only upload changed chunks.
- **Deduplication**: Hash each chunk. If the hash exists in S3, reference it instead of storing again.
- **Uploads**: Use pre-signed URLs for direct upload to S3. Server never handles file bytes.
- **Versioning**: Enable S3 versioning to track file history and enable restore.
- **Sharing**: Generate pre-signed URLs with appropriate expiration for shared files.

**S3 key structure:**

### 9.2 Video Streaming Platform (YouTube/Netflix)
Video platforms have distinct upload, processing, and delivery phases, each using S3 differently.
**Storage optimization:**
Apply lifecycle policies: transition videos not accessed in 90 days to Glacier Instant Retrieval (still fast enough for on-demand viewing, but 80% cheaper).

### 9.3 Handling Common Follow-up Questions

#### "What if uploads fail midway?"
Use multipart upload. Each part is independent. If part 5 of 10 fails, retry only part 5. The upload ID remains valid, so clients can resume even after reconnecting.

#### "How do you handle concurrent writes to the same file?"
S3 doesn't lock. Two options:
1. Enable versioning and reconcile after the fact (simpler)
2. Use DynamoDB with conditional writes as a coordination layer (stronger guarantees)

#### "What about latency for global users?"
CloudFront caches content at edge locations. For uploads, consider Transfer Acceleration (routes through CloudFront edge to AWS backbone). For metadata queries, replicate the DynamoDB table across regions.

#### "How do you estimate costs?"
Walk through the calculation:

### 9.4 Quick Reference for Interviews
| Topic | Key Numbers to Know |
| --- | --- |
| Max object size | 5 TB (use multipart for > 100 MB) |
| Durability | 99.999999999% (11 nines) |
| Availability | 99.99% (Standard) |
| Consistency | Strong read-after-write |
| Throughput per prefix | 3,500 writes + 5,500 reads per second |
| Storage classes | 7 (Standard → Deep Archive) |
| Encryption default | SSE-S3 (automatic since 2023) |
| Cost range | $0.023/GB (Standard) to $0.001/GB (Deep Archive) |

# Summary
S3 appears in nearly every system design interview involving file storage. Here's what to remember:
**When to use S3**: Unstructured data (files, images, videos), write-once-read-many workloads, situations where you need durability without operational overhead. S3 is not a database; don't use it for frequent small updates or when you need low-latency lookups.
**Cost optimization**: Storage class selection and lifecycle policies are where the money is. Moving cold data to Glacier can reduce costs by 95%. Mention this proactively in interviews.
**Performance at scale**: Understand the per-prefix limits (3,500 writes, 5,500 reads per second) and how to work around them by distributing keys across prefixes. Know that multipart upload is essential for large files.
**Security**: IAM policies, bucket policies, and Block Public Access work together. Enable Block Public Access by default; disable only when genuinely needed. Use pre-signed URLs for temporary access.
**Consistency**: S3 is strongly consistent for all operations since December 2020. You can rely on read-after-write consistency.
**Replication**: Cross-region replication for disaster recovery. Same-region replication for log aggregation or environment isolation. Replication Time Control when you need SLA guarantees.
The difference between a good interview answer and a great one is specificity. Don't just say "we'll use S3." Explain which storage class, what lifecycle policies, how you'll handle uploads, and what security configuration. This shows you've actually built systems, not just read about them.
# References
- [Amazon S3 Developer Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/) - Official AWS documentation covering all S3 features
- [Amazon S3 FAQs](https://aws.amazon.com/s3/faqs/) - Common questions about S3 capabilities and limits
- [S3 Strong Consistency Announcement](https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/) - Details on S3's consistency model update
- [AWS re:Invent 2023: Deep Dive on Amazon S3](https://www.youtube.com/watch?v=sYDJYqvNeXU) - Advanced S3 architecture and optimization
- [S3 Performance Guidelines](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html) - Best practices for high-throughput workloads

# Quiz

## S3 Quiz
Which description best matches S3 storage?