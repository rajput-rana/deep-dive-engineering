# Design Search Autocomplete System

## What is a Search Autocomplete System?

A search autocomplete system suggests possible queries to users as they type into a search bar. For example, typing "best re" might prompt completions like "best restaurants near me" or "best recipes for dinner." 
Autocomplete improves user experience by reducing typing effort, guiding queries, and surfacing popular or trending searches.
In this chapter, we will explore the **high-level design of a search autocomplete system.**
Let’s begin by clarifying the requirements.
# 1. Clarifying Requirements
Before diving into the design, it’s important to clarify assumptions and define scope. Here’s an example of how a candidate–interviewer discussion might go:
**Candidate:** Should the system suggest completions based only on historical queries or also on trending data?
**Interviewer:** Both. Suggestions should come from past queries, but trending searches should be prioritized.
**Candidate:** Do we need to personalize suggestions for each user?
**Interviewer:** Personalization is useful, but focus first on generic suggestions.
**Candidate:** Should suggestions update after each keystroke?
**Interviewer:** Yes, suggestions should update dynamically as the user types.
**Candidate:** How many suggestions should we return?
**Interviewer:** Assume 5–10 ranked suggestions per query.
**Candidate:** Do we need to support multiple languages?
**Interviewer:** Start with English, but the design should allow extensions.
**Candidate:** Should the system filter inappropriate or malicious queries?
**Interviewer:** Yes, filtering is required to maintain quality and safety.
**Candidate:** What about scale?
**Interviewer:** Assume millions of users and queries per second worldwide.
After gathering the details, we can summarize the key system requirements.

## 1.1 Functional Requirements
- **Top N Suggestions:** The system must return the top N (e.g., 5-10) most relevant suggestions for a given input prefix.
- **Dynamic Updates:** Suggestions should update instantly as the user types each character.
- **Ranking:** Support flexible ranking mechanisms based on popularity, recency, or personalization.
- **Typo Tolerance:** Ideally, the system should handle minor typos and offer corrections.
- **Multi-language Support:** The system should be able to provide suggestions for multiple languages or locales.

## 1.2 Non-Functional Requirements
- **Low Latency:** Suggestions must be returned within a very tight latency budget, typically < 100 ms per query.
- **High Scalability:** The system needs to scale to millions of active users and billions of queries per day, especially for popular platforms.
- **Fault Tolerance & High Availability:** The system should remain operational even if some components fail.
- **Near Real-time Updates:** The list of popular or trending terms should be updated frequently (e.g., hourly or daily) to reflect current trends.

### Scale Estimation
Assume we're building for a large-scale platform like Google Search or Amazon:
- 500 million daily active users
- Each user performs 10 searches per day on average
- Each search query generates 5-10 autocomplete requests (one per keystroke)
- Total: 25-50 billion autocomplete queries per day
- Peak QPS (queries per second): ~1 million QPS during peak hours
- Storage: 100 million unique search terms, with metadata

These numbers guide our architectural decisions around caching, sharding, and replication.
# 2. Understanding the Problem
At its core, "autocomplete" is about efficiently finding words that start with a given prefix and then intelligently ranking them.
It's a blend of efficient data retrieval and smart relevance scoring.
Technically, this means:
1. **Prefix Search:** The system needs a way to rapidly search for all terms that begin with the characters the user has typed so far.
2. **Relevance Ranking:** Once potential completions are found, they need to be ordered by how likely or useful they are to the user.
3. **Dynamic Updates:** The underlying data, especially popularity scores, must be refreshed regularly to stay relevant.
4. **Speed through Caching:** Popular queries and their results should be stored closer to the user for even faster access.

Here's a typical workflow:
The user types "sp," the client immediately sends a request, the service looks up matching terms, ranks them based on relevance signals, and returns the top suggestions. This entire round trip needs to complete before the user finishes typing the next character.
# 3. High-Level Architecture
Building an autocomplete system involves several interconnected components, each playing a crucial role.
Let's break down the main parts:
1. **Frontend / Client:** This is what the user interacts with (web browser, mobile app). It sends prefix queries to the backend with each keystroke.
2. **API Gateway:** Acts as the entry point for all client requests. It handles authentication, rate limiting, SSL termination, and routes requests to the appropriate backend service.
3. **Autocomplete Service:** The brain of our system. It's responsible for orchestrating the prefix lookup, fetching results from the data store, applying ranking algorithms, and interacting with the cache. This service will likely be stateless and highly scalable.
4. **Data Store:** This is where all the potential search terms, their frequencies, popularity scores, and other metadata are stored. This needs to be optimized for prefix searches.
5. **Cache Layer:** A critical component for low latency. It stores popular prefixes and their corresponding suggestions, preventing repeated lookups in the main data store.
6. **Search Logs:** Captures user interactions, click-through rates (CTR), and system performance metrics. This data is invaluable for improving ranking models and identifying performance bottlenecks.
7. **Indexing Pipeline:** A separate, asynchronous process that continuously ingests new data (search logs, trending topics), cleans it, computes scores, and builds/updates the data store and relevant indexes.

# 4. Design Deep Dive

## 4.1 Data Model and Storage Design
Designing an efficient autocomplete or search-suggestion system begins with choosing the right **data structure** for prefix lookups.
The ideal choice must allow rapid retrieval of all words that begin with a given sequence of characters. For instance, retrieving `"new york"`, `"new delhi"`, and `"new balance"` when the prefix `"new"` is typed.

### The Trie (Prefix Tree)
A **Trie** (pronounced “try”), or **Prefix Tree**, is the natural fit for this problem. It’s a tree-based data structure optimized for prefix matching.

#### How It Works
Each node in a Trie represents a **character**, and the path from the **root** to a **node** represents a prefix. When a path spells out a complete word, that node is marked as an **end of word**.
For example, consider inserting `"new york"`, `"new delhi"`, and `"new balance"`:

##### Key characteristics:
- **Efficient Prefix Search:** To find all words starting with "new", you traverse the Trie along the "n" path, then the "e" path, then the "w" path. From the 'w' node, you can traverse all its children to find "new york", "new delhi", and "new balance".
- **Time Complexity:** Lookups take O(m) time, where 'm' is the length of the prefix. This is incredibly fast, as it doesn't depend on the total number of words.
- **Space Efficiency:** Common prefixes share nodes, significantly reducing redundancy compared to storing entire words independently. The worst-case space complexity is **O(ALPHABET_SIZE × N × M)**, but with compression techniques like **radix trees** or **path compression**, this becomes manageable.
- **Metadata Storage:** Each node can also store metadata useful for ranking and personalization:

##### Code Representation:

## 4.2 Scaling Trie in a Distributed Environment
While a single-machine Trie works for small datasets, large-scale systems like Google Search or YouTube Autocomplete must manage **billions of entries**.
This requires **distributing the Trie** across multiple nodes while ensuring low latency, consistency, and fault tolerance.
Here are the main strategies to scale a Trie in a distributed setup:

### 1. Partitioning by Prefix (Sharding)
This is the most intuitive approach. We slice the Trie horizontally based on the first few characters of the search terms (typically the **first one or two characters**). 
Think of it like a physical dictionary split into several volumes: A-F, G-M, and so on.

#### How it Works:
Suppose our sharding scheme looks like this:
- **Shard 1:** Handles all prefixes starting with `a` through `f`.
- **Shard 2:** Handles all prefixes starting with `g` through `m`.
- **Shard 3:** Handles all prefixes starting with `n` through `s`.
- **Shard 4:** Handles all prefixes starting with `t` through `z`.

Here. the data is sliced based on the first character of the prefix. Each shard holds a completely independent, smaller Trie for its designated character range.
A **router** or **load balancer** sits in front of our Trie cluster. When a query like `"system design"` arrives, the router inspects the first character (`'s'`) and directs the request to the specific server (shard) responsible for that character range.

#### Pros:
- **Simplicity:** The routing logic is straightforward and easy to implement. You just need a simple mapping of character ranges to server addresses.
- **Data Locality:** All related terms (e.g., "system", "systems", "systematic") are guaranteed to be on the same shard, making searches within that prefix space very efficient.

#### Cons: The Hotspot Problem
The biggest drawback is **uneven load distribution**. The English language, and user search behavior, is not uniform. Prefixes starting with 's', 'c', or 'p' are far more common than those starting with 'x', 'y', or 'z'.
This creates "hotspots". Some shards will be overwhelmed with traffic while others sit idle.
Furthermore, **rebalancing is difficult**. If the `[n-s]` shard becomes too large and needs to be split, it's often a manual, disruptive process that requires careful planning to migrate data and update routing rules without causing downtime.

### 2. Hash-Based Partitioning (Using Consistent Hashing)
To overcome the hotspot problem, we can use a more sophisticated approach that distributes data randomly but consistently: **consistent hashing**.

#### How it Works:
Instead of looking at the character, we compute a hash of the prefix (or its first few characters) and map that hash value to a point on a virtual ring. Our server nodes are also mapped to points on this same ring.
To find which shard a prefix belongs to, we hash the prefix and walk clockwise around the ring until we find the next server.
`shard_id = consistent_hash(prefix[:k])`
Here, `k` is the number of characters we use for hashing (e.g., the first 2 or 3).

#### Pros:
- **Even Distribution:** A good hash function distributes prefixes uniformly across all shards, eliminating the hotspots seen in range-based sharding.
- **Easy Scaling:** When a new server is added, it's placed on the ring and only takes over a small portion of the keys from its immediate neighbor. This makes adding or removing nodes far less disruptive than re-partitioning entire character ranges.

#### Cons: Loss of Prefix Locality
While great for distribution, naive hashing breaks a key assumption of autocomplete. If we hash the entire prefix, `hash("new")` and `hash("news")` could easily land on different shards.
This means a query for "new" on Shard A wouldn't be able to find the suggestion "news" on Shard B.

#### Hybrid Approach
We can get the best of both worlds by **hashing only the first few characters** (e.g., the first 2 or 3).
- `hash("ne")` routes queries for "new", "news", and "netflix" to the **same shard**.
- `hash("sp")` routes queries for "spotify" and "sports" to the **same shard**.

This approach provides excellent load distribution while preserving the prefix locality needed for the Trie to function correctly.

### 3. Geo-Distribution and Federation
For a truly global application, performance isn't just about server capacity, it's about the speed of light. A user in India querying a server in North America will always experience high latency. 
The solution is to bring the data closer to the user through **geo-distribution**.

#### How it Works:
We deploy independent Trie clusters in multiple geographic regions (e.g., `us-east-1`, `eu-west-1`, `ap-south-1`). Each cluster holds suggestions most relevant to its region.
An **Aggregation Layer** (or Federation Service) sits on top. It's responsible for:
1. Query regional Tries in parallel.
2. Merge and rank results based on frequency, recency, and personalization.
3. In some cases, it might query multiple clusters (e.g., the local one and a global one) and merge the results, prioritizing local suggestions (e.g., prioritize “New Delhi” over “New York” for Indian users).

#### Benefits:
- **Massively Reduced Latency:** Users get responses from a data center that's physically close to them, resulting in a much faster experience.
- **Regional Relevance:** The system can rank suggestions based on local context. A search for "football" will prioritize "American Football" in the US cluster and "Soccer" in the EU cluster. "Cricket" suggestions would be far more prominent in the APAC cluster.
- **High Availability & Fault Tolerance:** If the entire `us-east-1` region goes down, the aggregation layer can intelligently reroute traffic to another region, ensuring the service remains available.

## 4.3 Optimizing Hot Prefixes
In real-world autocomplete systems, **a small number of prefixes dominate traffic**.
For instance, in an e-commerce search system:
- “iphone” and “samsung” may account for 30% of queries.
- Thousands of other prefixes occur rarely.

The most straightforward and effective optimization is to cache the results for the most popular prefixes. We identify the "hot" prefixes—the ones users search for thousands of times a minute (like "new", "weather", "cricket")—and store their top N suggestions in a fast, in-memory key-value store like **Redis** or **Memcached**.
The goal is simple:** Serve as many autocomplete requests as possible directly from memory.**

#### How it Works
The system operates on a simple key-value principle:
- **Key:** The search prefix string (e.g., `"game of"`).
- **Value:** A pre-computed, ranked list of the top N suggestions (e.g., `["game of thrones", "game of thrones cast", "game of life"]`).

When a request for `"game of"` arrives, the service first checks Redis. If the key exists (a **cache hit**), it grabs the list and returns it instantly. The slower, more expensive Trie lookup is completely bypassed.

### Two-Level Caching
A **two-level cache hierarchy** combines ultra-fast local memory access with shared global consistency — just like modern CPU cache hierarchies (L1/L2).

#### Level 1: In-Memory Cache (Per Node)
Each instance of the **Autocomplete Service** maintains a **local cache** often implemented using an **LRU (Least Recently Used)** policy.
- Stores only the most frequently accessed prefixes for that particular node.
- Reduces network calls to Redis for repeated queries handled by the same server.
- Perfect for ultra-low latency — a local memory lookup is typically **<1ms**.

Example (in-memory LRU cache):

#### Level 2: Distributed Central Cache (Redis Cluster)
A centralized **Redis cluster** acts as a global, shared cache across all Autocomplete Service nodes.
- If a prefix is missing in the node’s L1 cache, the system queries the Redis cluster.
- Redis holds a much larger set of prefixes, shared across nodes.
- Ensures **consistency** — all nodes see the same updated results.
- Reduces load on the Trie or primary datastore.

## 4.4 Indexing and Ingestion Pipeline
An **autocomplete system** is only as smart as the **data** behind it.
Even the most optimized Trie or caching layer can’t help if the underlying suggestions are stale, irrelevant, or incomplete.
To stay current, the system needs a continuous flow of fresh, clean, and ranked data. This is the job of the **Indexing and Ingestion Pipeline**.
The Indexing and Ingestion Pipeline is responsible for:
- **Collecting** new and trending terms from multiple sources.
- **Cleaning and normalizing** this data for consistency.
- **Scoring and ranking** terms based on popularity and recency.
- **Indexing** these terms into the primary lookup structure (Trie).
- **Distributing** updates across all autocomplete service nodes.

This pipeline usually runs **asynchronously and periodically,** for example:
- Every few minutes for real-time trending data.
- Every few hours or nightly for bulk log processing.

### Data Sources
The quality of autocomplete suggestions depends heavily on the **breadth and freshness of input data**.
Most systems aggregate terms from a combination of the following sources:
- **Historical Search Logs:** The most important source. Captures what users have actually searched for.
- **Trending Queries:** Real-time data from analytics, news, or social media trends. Detects sudden spikes in interest.
- **Domain-Specific Dictionaries**: Specialized datasets relevant to your application domain.. For e-commerce, this might be a product catalog; for a knowledge base, it could be article titles.
- **Human-Curated Lists**: Manually adding important terms or blocking undesirable ones. Ensures high-value terms are included and inappropriate ones are excluded.

### Steps in the Pipeline
Let’s break down the pipeline into its major stages.

#### 1. Data Ingestion
The first step is to **collect raw data** from various input sources — historical logs, catalogs, feeds, etc.
Depending on the system scale, ingestion can happen in two modes:
- **Batch Ingestion** (e.g., hourly processing via Spark or Flink)
- **Streaming Ingestion** (e.g., real-time feeds through Kafka)

**Example:**
Kafka topics ingest continuous query logs:
- `search_events` → user queries
- `click_events` → user clicks
- `trend_signals` → trending keywords

Batch jobs periodically consume and aggregate these streams.

#### 2. Data Cleaning and Tokenization
Raw user queries are messy. They contain typos, mixed casing, redundant spaces, and punctuation.
The cleaning stage standardizes and normalizes terms to ensure **uniform indexing**.

##### Common Cleaning Steps:
- Lowercasing all text. Example: “New York” → “new york”
- Removing punctuation and special characters. Example: “new-york!” → “new york”
- Removing stop words. Example: “the new iphone” → “new iphone”
- Tokenizing multi-word phrases. Example: “new york city” → `[new, york, city]`

For **autocomplete**, we often **avoid stemming** since users expect literal completions (e.g., “running shoes” ≠ “run shoes”).

#### 3. Popularity and Ranking Score Computation
Once terms are cleaned, the pipeline computes a **score** for each term that represents its relevance and importance.
This score determines how suggestions are ordered during autocomplete.
**Basic Formula (Weighted Frequency):**
score(term)=α×freqrecent​+β×freqhistorical​+γ×click_through_rate
- **freq_recent**: Count of searches in the last few days.
- **freq_historical**: Long-term frequency.
- **click_through_rate**: How often users clicked on results for that term.
- **α, β, γ**: Tunable weights (e.g., α=0.5, β=0.3, γ=0.2).

#### 4. Indexing
Once the terms are ranked, we update the **main lookup structure** typically a **Trie**, **inverted index**, or **search tree**.
- Each term is inserted into the Trie along its character path.
- Leaf or terminal nodes store metadata:
- Prefix nodes may also cache the **top-N suggestions** to speed up lookups.

#### 5. Distribution and Deployment
After the index is updated, the new version must be **distributed** across all Autocomplete Service instances.
There are typically two approaches:

##### A. Push-Based Deployment
- The Indexing Service pushes serialized index files (e.g., compressed Trie snapshots) to all nodes.
- Nodes hot-swap the index in memory without downtime.

##### B. Pull-Based Deployment
- Each node periodically polls a central store (e.g., S3, Cassandra) for the latest index version.
- Upon detecting a new version, it asynchronously downloads and loads it.

## 4.5 Query Flow (Autocomplete Lookup)
Now that we have our data structured and indexed, let's trace how a user's query gets processed in real-time.
1. **User Types a Prefix:** As the user types, say "spo," the frontend client debounces input (to avoid sending a request for every single keystroke) and then sends a request.
2. **Client Sends Request:** An HTTP GET request is sent to the API Gateway: `/autocomplete?q=spo`.
3. **API Gateway Routes:** The API Gateway forwards the request to an available instance of the Autocomplete Service.
4. **Service Checks Cache:** The Autocomplete Service first checks its distributed cache (e.g., Redis) for suggestions associated with the prefix "spo." This is the fastest path.
5. **Cache Hit:** If found, the cached, ranked suggestions are immediately returned to the client. This is the ideal scenario for popular prefixes.
6. **Cache Miss:** If not found in the cache:
7. **Frontend Displays:** The frontend receives the suggestions and displays them to the user.

## 4.6 Ranking and Personalization
Matching prefixes is only half the job. When a user types `"apple"`, there might be **hundreds of possible completions** — company names, recipes, products, locations, news articles, and more.
A great autocomplete system doesn’t just *find* matches, it *prioritizes* the **most relevant** ones.
This is where **ranking and scoring** come into play.
Each autocomplete suggestion is scored using multiple **signals** that capture different aspects of relevance — popularity, freshness, engagement, and personalization.
We can represent the final score as a **weighted sum** of these signals:
Where:
- α,β,γ,δ are **weights** determining the relative importance of each signal.
- These weights are **tuned via A/B testing** or **machine learning models** to optimize engagement metrics.

Autocomplete systems shouldn’t feel generic, they should feel *personal*. When two users type `"apple"`, one might want **“Apple stock price”**, while another might mean **“apple pie recipe”**.
To move beyond generic suggestions, we need to make the autocomplete system smarter by understanding the user and their immediate context.
To deliver such experiences, we need the system to **understand who the user is** and **what context they’re in**.
Let’s explore the most common signals used to personalize autocomplete results.

#### 1. User’s Search History
User search patterns often reveal long-term interests and intent.
If a user frequently searches for `"vegan recipes"`, then typing `"vegan"` again should instantly suggest:
- “vegan pasta recipe”
- “vegan desserts”
- “vegan restaurants near me”

Instead of generic results like `"vegan shoes"` or `"vegan cosmetics"`.

#### 2. Location Awareness
Where a user is physically located can dramatically alter the meaning of a query.
Typing `"restaurants"` in:
- **New York** → “Best restaurants NYC”, “Pizza near Times Square”
- **Rome** → “Best pasta places in Rome”, “Trattoria near me”

#### 3. Time of Day / Week
Time-based context can make certain suggestions more relevant.

### Implementing Personalization

#### 1. User Profiles and Embeddings
Each user can be represented by a **vector embedding,** a numerical representation of their interests and behaviors.
**Example:**
A user frequently searching for tech products might have an embedding like:
These embeddings can be stored in a **feature store** (e.g., Redis, Cassandra, Feast) and fetched quickly during query time.

#### 2. Session-Level Personalization
Not all personalization comes from long-term behavior. Context evolves even within a session.
**Example:**
User searches: `system design`
User then types: `sca...`
The system, remembering the session's context ("system design"), heavily boosts **"scalability," "scaling,"** and **"ScyllaDB".**
**Implementation:**
- Maintain a **session context state** (stored in memory or Redis).
- Use recent queries to dynamically re-weight ranking signals

## 4.7 Handling Typos and Fuzzy Matching
No matter how advanced your system is, users will **make mistakes while typing** — extra letters, missing letters, or simple transpositions like typing `"googel"` instead of `"google"`.A truly intelligent autocomplete system must be **forgiving** of such typos, helping users find what they meant, not just what they typed.
This is where **fuzzy matching** comes into play. It enabes the system to suggest relevant results even when the input doesn’t exactly match stored prefixes.
Let’s explore the key techniques used to enable typo tolerance in autocomplete systems.

### 1. Levenshtein Distance
**Levenshtein Distance** measures how many single-character edits (insertions, deletions, substitutions) are needed to transform one string into another.
We can use this metric to find all terms in our index within a distance of **1 or 2** from the user’s input.
- If the user types `"googel"`, we check for words in the index where `Levenshtein ≤ 2`.
- `"google"` is identified as the best candidate correction.

### 2. BK-Trees (Burkhard–Keller Trees)
While Levenshtein distance works conceptually, computing it for every possible word in a large dictionary is expensive.
To scale, we use a **BK-Tree**, a tree-based data structure built on edit distances.

#### How it Works
- Each node in the tree represents a word.
- Edges between nodes are labeled with the **Levenshtein distance** between them.
- During a query, we traverse only branches where the edit distance is within the allowed threshold (e.g., ≤ 2).

##### Example
If a user types `"bokk"`, the BK-tree allows the search to **prune** most branches quickly and return `"book"`, `"cook"`, and `"look"` efficiently.

##### Complexity
- Query time: **O(log N)** for typical edit thresholds.
- Much faster than brute-force checking all dictionary words.

BK-Trees are especially effective when the dictionary is static or updated infrequently. Common in autocomplete systems with pre-indexed search terms.

#### 3. N-gram (Character-based) Models
Instead of looking at whole words, we can break terms into **character sequences** (n-grams).
For example, using **bigrams (2-character chunks)**:
Now, we can calculate the **similarity** between two terms based on **shared n-grams**:
“apple” is the closest match since it shares the most overlapping n-grams with “aple”.
N-gram similarity is fast and works well when minor letter-level errors occur, making it ideal for **typo detection** and **fuzzy prefix search** in autocomplete.

#### 4. Spell Correction Models
As we scale, rule-based methods alone struggle to handle:
- Complex typos (`“gogle”` vs `“google”`)
- Phonetic mistakes (`“nite”` vs `“night”`)
- Multilingual inputs (`“gogle mapas”`)

That’s where **machine learning–based spell correction models** come in.
These models learn spelling errors from **real-world query logs** and use contextual or statistical reasoning to correct them.

##### Example: Context-aware Correction
These models can be integrated with the autocomplete pipeline to correct prefixes **on the fly**.
# Quiz

## Design Search Autocomplete System Quiz
For an autocomplete service, which data structure is most directly optimized for prefix lookups like finding all queries starting with "new"?